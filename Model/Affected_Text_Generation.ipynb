{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import json\n",
    "from operator import add\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import trange\n",
    "from transformers import GPT2Tokenizer\n",
    "from transformers.file_utils import cached_path\n",
    "from transformers.modeling_gpt2 import GPT2LMHeadModel\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Required Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resultContainer = {\n",
    "    \"text\":[]\n",
    "}\n",
    "\n",
    "class ClassificationHead(torch.nn.Module):\n",
    "    \"\"\"Classification Head for  transformer encoders\"\"\"\n",
    "\n",
    "    def __init__(self, class_size, embed_size):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.class_size = class_size\n",
    "        self.embed_size = embed_size\n",
    "        # self.mlp1 = torch.nn.Linear(embed_size, embed_size)\n",
    "        # self.mlp2 = (torch.nn.Linear(embed_size, class_size))\n",
    "        self.mlp = torch.nn.Linear(embed_size, class_size)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        # hidden_state = F.relu(self.mlp1(hidden_state))\n",
    "        # hidden_state = self.mlp2(hidden_state)\n",
    "        logits = self.mlp(hidden_state)\n",
    "        return logits\n",
    "\n",
    "PPLM_BOW = 1\n",
    "PPLM_DISCRIM = 2\n",
    "PPLM_BOW_DISCRIM = 3\n",
    "BOW_AFFECT = 4\n",
    "SMALL_CONST = 1e-15\n",
    "BIG_CONST = 1e10\n",
    "\n",
    "QUIET = 0\n",
    "REGULAR = 1\n",
    "VERBOSE = 2\n",
    "VERY_VERBOSE = 3\n",
    "VERBOSITY_LEVELS = {\n",
    "    'quiet': QUIET,\n",
    "    'regular': REGULAR,\n",
    "    'verbose': VERBOSE,\n",
    "    'very_verbose': VERY_VERBOSE,\n",
    "}\n",
    "\n",
    "BAG_OF_WORDS_ARCHIVE_MAP = {\n",
    "    'legal': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/legal.txt\",\n",
    "    'military': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/military.txt\",\n",
    "    'monsters': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/monsters.txt\",\n",
    "    'politics': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/politics.txt\",\n",
    "    'positive_words': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/positive_words.txt\",\n",
    "    'religion': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/religion.txt\",\n",
    "    'science': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/science.txt\",\n",
    "    'space': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/space.txt\",\n",
    "    'technology': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/technology.txt\",\n",
    "}\n",
    "\n",
    "DISCRIMINATOR_MODELS_PARAMS = {\n",
    "    \"clickbait\": {\n",
    "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/clickbait_classifier_head.pt\",\n",
    "        \"class_size\": 2,\n",
    "        \"embed_size\": 1024,\n",
    "        \"class_vocab\": {\"non_clickbait\": 0, \"clickbait\": 1},\n",
    "        \"default_class\": 1,\n",
    "        \"pretrained_model\": \"gpt2-medium\",\n",
    "    },\n",
    "    \"sentiment\": {\n",
    "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/SST_classifier_head.pt\",\n",
    "        \"class_size\": 5,\n",
    "        \"embed_size\": 1024,\n",
    "        \"class_vocab\": {\"very_positive\": 2, \"very_negative\": 3},\n",
    "        \"default_class\": 3,\n",
    "        \"pretrained_model\": \"gpt2-medium\",\n",
    "    },\n",
    "}\n",
    "\n",
    "def to_var(x, requires_grad=False, volatile=False, device='cuda'):\n",
    "    if torch.cuda.is_available() and device == 'cuda':\n",
    "        x = x.cuda()\n",
    "    elif device != 'cuda':\n",
    "        x = x.to(device)\n",
    "    return Variable(x, requires_grad=requires_grad, volatile=volatile)\n",
    "\n",
    "\n",
    "def top_k_filter(logits, k, probs=False):\n",
    "    \"\"\"\n",
    "    Masks everything but the k top entries as -infinity (1e10).\n",
    "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
    "    sum of the denominator.\n",
    "    \"\"\"\n",
    "    if k == 0:\n",
    "        return logits\n",
    "    else:\n",
    "        values = torch.topk(logits, k)[0]\n",
    "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
    "        if probs:\n",
    "            return torch.where(logits < batch_mins,\n",
    "                               torch.ones_like(logits) * 0.0, logits)\n",
    "        return torch.where(logits < batch_mins,\n",
    "                           torch.ones_like(logits) * -BIG_CONST,\n",
    "                           logits)\n",
    "\n",
    "def gaussian(x, mu, sig):\n",
    "  x = np.array(x)\n",
    "  return list(np.exp(-0.5*((x-mu)/sig)**2)/(sig*(2*np.pi)**0.5))\n",
    "\n",
    "def perturb_past(\n",
    "        past,\n",
    "        model,\n",
    "        last,\n",
    "        affect_weight=0.2,\n",
    "        unpert_past=None,\n",
    "        unpert_logits=None,\n",
    "        accumulated_hidden=None,\n",
    "        grad_norms=None,\n",
    "        stepsize=0.01,\n",
    "        one_hot_bows_vectors=None,\n",
    "        one_hot_bows_affect=None,\n",
    "        affect_int = None,\n",
    "        knob = None,\n",
    "        classifier=None,\n",
    "        class_label=None,\n",
    "        loss_type=0,\n",
    "        num_iterations=3,\n",
    "        horizon_length=1,\n",
    "        window_length=0,\n",
    "        decay=False,\n",
    "        gamma=1.5,\n",
    "        score_scale=0.01,\n",
    "        device='cuda',\n",
    "        verbosity_level=REGULAR,\n",
    "        beta1=0.6,\n",
    "        end_lr = 0.5,\n",
    "        N = 15,\n",
    "        power = 2\n",
    "):\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if accumulated_hidden is None:\n",
    "        accumulated_hidden = 0\n",
    "    if decay:\n",
    "        decay_mask = torch.arange(\n",
    "            0.,\n",
    "            1.0 + SMALL_CONST,\n",
    "            1.0 / (window_length)\n",
    "        )[1:]\n",
    "    else:\n",
    "        decay_mask = 1.0\n",
    "    # Generate a mask is gradient perturbated is based on a past window\n",
    "    _, _, _, curr_length, _ = past[0].shape\n",
    "\n",
    "    if curr_length > window_length and window_length > 0:\n",
    "        ones_key_val_shape = (\n",
    "                tuple(past[0].shape[:-2])\n",
    "                + tuple([window_length])\n",
    "                + tuple(past[0].shape[-1:])\n",
    "        )\n",
    "\n",
    "        zeros_key_val_shape = (\n",
    "                tuple(past[0].shape[:-2])\n",
    "                + tuple([curr_length - window_length])\n",
    "                + tuple(past[0].shape[-1:])\n",
    "        )\n",
    "\n",
    "        ones_mask = torch.ones(ones_key_val_shape)\n",
    "        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n",
    "        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n",
    "\n",
    "        window_mask = torch.cat(\n",
    "            (ones_mask, torch.zeros(zeros_key_val_shape)),\n",
    "            dim=-2\n",
    "        ).to(device)\n",
    "    else:\n",
    "        window_mask = torch.ones_like(past[0]).to(device)\n",
    "\n",
    "    # accumulate perturbations for num_iterations\n",
    "    loss_per_iter = []\n",
    "    new_accumulated_hidden = None\n",
    "    for i in range(1,num_iterations+1):\n",
    "        if verbosity_level >= VERBOSE:\n",
    "            print(\"Iteration \", i + 1)\n",
    "        past = tuple([\n",
    "            to_var(p_, requires_grad=True, device=device)\n",
    "            for p_ in past\n",
    "        ])\n",
    "        # Compute hidden using perturbed past \n",
    "        perturbed_past = past\n",
    "        _, _, _, curr_length, _ = perturbed_past[0].shape\n",
    "        all_logits, _, all_hidden = model(last, past_key_values=perturbed_past)\n",
    "        hidden = all_hidden[-1]\n",
    "        new_accumulated_hidden = accumulated_hidden + torch.sum(\n",
    "            hidden,\n",
    "            dim=1\n",
    "        ).detach()\n",
    "        \n",
    "        logits = all_logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        loss = 0.0\n",
    "        loss_list = []\n",
    "        # Emotion and Topic Loss\n",
    "        if loss_type == PPLM_BOW or loss_type == BOW_AFFECT:\n",
    "            for one_hot_bow in one_hot_bows_vectors:\n",
    "                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
    "                #print(type(bow_logits))\n",
    "                bow_loss = -torch.log(torch.sum(bow_logits))\n",
    "                #print(bow_loss)\n",
    "                loss +=  bow_loss\n",
    "                loss_list.append(bow_loss)\n",
    "            if loss_type == BOW_AFFECT:\n",
    "              for one_hot_bow in one_hot_bows_affect:\n",
    "                  bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
    "                 # print(bow_logits.size(), torch.FloatTensor(affect_int).size())\n",
    "                  bow_loss = -torch.log(torch.matmul(bow_logits, torch.t(torch.FloatTensor(gaussian(affect_int, knob, .1)).to(device))))\n",
    "                  # print(bow_loss)\n",
    "\n",
    "                  loss += affect_weight * bow_loss[0]\n",
    "                  loss_list.append(bow_loss)\n",
    "            if verbosity_level >= VERY_VERBOSE:\n",
    "                print(\" pplm_bow_loss:\", loss.data.cpu().numpy())\n",
    "\n",
    "        \n",
    "        # Proposed Loss        \n",
    "        score_loss = 0.0\n",
    "\n",
    "        if score_scale > 0.0 :\n",
    "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
    "            score = torch.sum(torch.mul(unpert_probs,probs)/(torch.norm(probs)*torch.norm(unpert_probs)))\n",
    "            score_loss = -score_scale *  score\n",
    "            loss += score_loss\n",
    "            if verbosity_level >= VERY_VERBOSE:\n",
    "                print(' Score_loss', score_loss.data.cpu().numpy())\n",
    "        \n",
    "        \n",
    "        loss_per_iter.append(loss.data.cpu().numpy())\n",
    "        if verbosity_level >= VERBOSE:\n",
    "            print(' Total_loss', (loss).data.cpu().numpy())\n",
    "        \n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # calculate gradient norms\n",
    "        if grad_norms is not None and loss_type == PPLM_BOW:\n",
    "            grad_norms = [\n",
    "                torch.max(grad_norms[index], torch.norm(p_.grad * window_mask))\n",
    "                for index, p_ in enumerate(perturbed_past)\n",
    "            ]\n",
    "        else:\n",
    "            grad_norms = [\n",
    "                (torch.norm(p_.grad * window_mask) + SMALL_CONST)\n",
    "                for index, p_ in enumerate(perturbed_past)\n",
    "            ]\n",
    "\n",
    "        # Initialise V_t\n",
    "        m_0 = tuple([ torch.tensor(np.zeros(p.shape).astype(\"float32\")) for p in perturbed_past ])\n",
    "        grad = tuple([\n",
    "            (p_.grad * window_mask/ grad_norms[index] ** gamma ).data.cpu().numpy()\n",
    "            for index, p_ in enumerate(perturbed_past)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Learning rate Decay\n",
    "        initial_lr = stepsize - end_lr\n",
    "        lr = initial_lr * ((num_iterations - i)/(num_iterations - N)) ** power # Polynomial Decay\n",
    "        \n",
    "\n",
    "        # First order moment Update\n",
    "        r_t = beta1/(1 - (beta1)**i)\n",
    "        r_t_1 = (1 - beta1)/(1 - (beta1)**i)\n",
    "        if i ==1:\n",
    "            m_t = [(r_t*momentum) + (r_t_1*grad[index]) for index , momentum in enumerate(m_0)] \n",
    "        else:\n",
    "            m_t = [(r_t*momentum) + (r_t_1*grad[index]) for index , momentum in enumerate(m_t)]\n",
    "        \n",
    "        # perturbing the past\n",
    "        past = [torch.tensor(perturbed_past[index].detach() - lr * m_t[index].to(device)) for index , p_ in enumerate(perturbed_past)]\n",
    "\n",
    "        # reset gradients, just to make sure\n",
    "        for p_ in perturbed_past:\n",
    "            p_.grad.data.zero_()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    pert_past = perturbed_past\n",
    "    return pert_past, new_accumulated_hidden, grad_norms, loss_per_iter\n",
    "\n",
    "\n",
    "def get_classifier(\n",
    "        name: Optional[str],\n",
    "        class_label: Union[str, int],\n",
    "        device: str,\n",
    "        verbosity_level: int = REGULAR\n",
    ") -> Tuple[Optional[ClassificationHead], Optional[int]]:\n",
    "    if name is None:\n",
    "        return None, None\n",
    "\n",
    "    params = DISCRIMINATOR_MODELS_PARAMS[name]\n",
    "    classifier = ClassificationHead(\n",
    "        class_size=params['class_size'],\n",
    "        embed_size=params['embed_size']\n",
    "    ).to(device)\n",
    "    if \"url\" in params:\n",
    "        resolved_archive_file = cached_path(params[\"url\"])\n",
    "    elif \"path\" in params:\n",
    "        resolved_archive_file = params[\"path\"]\n",
    "    else:\n",
    "        raise ValueError(\"Either url or path have to be specified \"\n",
    "                         \"in the discriminator model parameters\")\n",
    "    classifier.load_state_dict(\n",
    "        torch.load(resolved_archive_file, map_location=device))\n",
    "    classifier.eval()\n",
    "\n",
    "    if isinstance(class_label, str):\n",
    "        if class_label in params[\"class_vocab\"]:\n",
    "            label_id = params[\"class_vocab\"][class_label]\n",
    "        else:\n",
    "            label_id = params[\"default_class\"]\n",
    "            if verbosity_level >= REGULAR:\n",
    "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
    "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
    "                print(\"using default class {}\".format(label_id))\n",
    "\n",
    "    elif isinstance(class_label, int):\n",
    "        if class_label in set(params[\"class_vocab\"].values()):\n",
    "            label_id = class_label\n",
    "        else:\n",
    "            label_id = params[\"default_class\"]\n",
    "            if verbosity_level >= REGULAR:\n",
    "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
    "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
    "                print(\"using default class {}\".format(label_id))\n",
    "\n",
    "    else:\n",
    "        label_id = params[\"default_class\"]\n",
    "\n",
    "    return classifier, label_id\n",
    "\n",
    "\n",
    "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> \\\n",
    "        List[List[List[int]]]:\n",
    "    bow_indices = []\n",
    "    for id_or_path in bag_of_words_ids_or_paths:\n",
    "        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n",
    "            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n",
    "        else:\n",
    "            filepath = id_or_path\n",
    "        with open(filepath, \"r\") as f:\n",
    "            words = f.read().strip().split(\"\\n\")\n",
    "        bow_indices.append(\n",
    "            [tokenizer.encode(word.strip(),\n",
    "                              add_prefix_space=True,\n",
    "                              add_special_tokens=False)\n",
    "             for word in words])\n",
    "    return bow_indices\n",
    "\n",
    "def get_affect_words_and_int (affect_class):\n",
    "  emotions = \"https://raw.githubusercontent.com/ishikasingh/Affective-text-gen/master/NRC-Emotion-Intensity-Lexicon-v1.txt\"\n",
    "  filepath = cached_path(emotions)\n",
    "  with open(filepath, \"r\") as f:\n",
    "      words = f.read().strip().split(\"\\n\")[1:]\n",
    "  words = [w.split(\"\\t\") for w in words]\n",
    "  return [w[0] for w in words if w[1] == affect_class], [float(w[-1]) for w in words if w[1] == affect_class]\n",
    "\n",
    "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n",
    "    if bow_indices is None:\n",
    "        return None\n",
    "\n",
    "    one_hot_bows_vectors = []\n",
    "    for single_bow in bow_indices:\n",
    "        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n",
    "\n",
    "        single_bow = torch.tensor(single_bow).to(device)\n",
    "        num_words = single_bow.shape[0]\n",
    "        # print(num_words)\n",
    "        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n",
    "        one_hot_bow.scatter_(1, single_bow, 1)\n",
    "        one_hot_bows_vectors.append(one_hot_bow)\n",
    "    return one_hot_bows_vectors\n",
    "\n",
    "def build_bows_one_hot_vectors_aff(bow_indices,affect_int, tokenizer, device='cuda'):\n",
    "    if bow_indices is None or affect_int is None:\n",
    "        return None, None\n",
    "\n",
    "    one_hot_bows_vectors = []\n",
    "    # print(np.array(bow_indices).shape)\n",
    "    for single_bow in bow_indices:\n",
    "        zipped = [[single_bow[i], affect_int[i]] for i in range(len(single_bow))]\n",
    "        single_bow_int = list(filter(lambda x: len(x[0]) <= 1, zipped))\n",
    "        single_bow = [single_bow_int[i][0] for i in range(len(single_bow_int)) ]\n",
    "        affect_ints = [single_bow_int[i][1] for i in range(len(single_bow_int)) ]\n",
    "        # print(single_bow, affect_ints)\n",
    "        # print(len(single_bow), len(affect_ints))\n",
    "        single_bow = torch.tensor(single_bow).to(device)\n",
    "        num_words = single_bow.shape[0]\n",
    "        # print(num_words)\n",
    "        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n",
    "        one_hot_bow.scatter_(1, single_bow, 1)\n",
    "        one_hot_bows_vectors.append(one_hot_bow)\n",
    "    return one_hot_bows_vectors, affect_ints\n",
    "\n",
    "\n",
    "\n",
    "def full_text_generation(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        affect_weight=0.2,\n",
    "        knob = None,\n",
    "        context=None,\n",
    "        num_samples=1,\n",
    "        device=\"cuda\",\n",
    "        bag_of_words=None,\n",
    "        bag_of_words_affect=None,\n",
    "        discrim=None,\n",
    "        class_label=None,\n",
    "        length=100,\n",
    "        stepsize=0.02,\n",
    "        temperature=1.0,\n",
    "        top_k=10,\n",
    "        sample=True,\n",
    "        num_iterations=3,\n",
    "        grad_length=10000,\n",
    "        horizon_length=1,\n",
    "        window_length=0,\n",
    "        decay=False,\n",
    "        gamma=1.5,\n",
    "        gm_scale=0.9,\n",
    "        score_scale=0.01,\n",
    "        verbosity_level=REGULAR,\n",
    "        beta1=0.6,\n",
    "        end_lr = 0.5,\n",
    "        N = 15,\n",
    "        power = 2,\n",
    "        **kwargs\n",
    "):\n",
    "    classifier, class_id = get_classifier(discrim, class_label, device)\n",
    "    # print(\"bog is here\", bag_of_words)\n",
    "    # print(\"affect is: \", bag_of_words_affect)\n",
    "    bow_indices = []\n",
    "    bow_indices_affect = []\n",
    "    if bag_of_words:\n",
    "      bow_indices = get_bag_of_words_indices(bag_of_words.split(\";\"), tokenizer)\n",
    "    if bag_of_words_affect: \n",
    "      affect_words, affect_int = get_affect_words_and_int(bag_of_words_affect)\n",
    "      bow_indices_affect.append([tokenizer.encode(word.strip(),add_prefix_space=True, add_special_tokens=False)for word in affect_words])\n",
    "    loss_type = PPLM_BOW\n",
    "    if bag_of_words_affect:\n",
    "      loss_type = BOW_AFFECT\n",
    "\n",
    "    unpert_gen_tok_text, _, _ = generate_text_pplm(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        context=context,\n",
    "        device=device,\n",
    "        length=length,\n",
    "        sample=sample,\n",
    "        perturb=False,\n",
    "        verbosity_level=verbosity_level\n",
    "    )\n",
    "\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    pert_gen_tok_texts = []\n",
    "    discrim_losses = []\n",
    "    losses_in_time = []\n",
    "    print(\"After Perturbation\")\n",
    "    for i in range(num_samples):\n",
    "        pert_gen_tok_text, discrim_loss, loss_in_time = generate_text_pplm(\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            affect_weight=affect_weight,\n",
    "            context=context,\n",
    "            device=device,\n",
    "            perturb=True,\n",
    "            bow_indices=bow_indices,\n",
    "            bow_indices_affect=bow_indices_affect,\n",
    "            affect_int = affect_int,\n",
    "            knob = knob,\n",
    "            classifier=classifier,\n",
    "            class_label=class_id,\n",
    "            loss_type=loss_type,\n",
    "            length=length,\n",
    "            stepsize=stepsize,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            sample=sample,\n",
    "            num_iterations=num_iterations,\n",
    "            grad_length=grad_length,\n",
    "            horizon_length=horizon_length,\n",
    "            window_length=window_length,\n",
    "            decay=decay,\n",
    "            gamma=gamma,\n",
    "            gm_scale=gm_scale,\n",
    "            score_scale=score_scale,\n",
    "            verbosity_level=verbosity_level,\n",
    "            beta1=beta1,\n",
    "            end_lr = end_lr,\n",
    "            N = N,\n",
    "            power = power\n",
    "        )\n",
    "        pert_gen_tok_texts.append(pert_gen_tok_text)\n",
    "        if classifier is not None:\n",
    "            discrim_losses.append(discrim_loss.data.cpu().numpy())\n",
    "        losses_in_time.append(loss_in_time)\n",
    "\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
    "\n",
    "\n",
    "def get_affect_words_and_int1 (affect_class):\n",
    "  emotions = \"https://raw.githubusercontent.com/ishikasingh/Affective-text-gen/master/NRC-AffectIntensity-Lexicon.txt\"\n",
    "  filepath = cached_path(emotions)\n",
    "  with open(filepath, \"r\") as f:\n",
    "      words = f.read().strip().split(\"\\n\")[37:]\n",
    "  words = [w.split(\"\\t\") for w in words]\n",
    "  return [w[0] for w in words if w[-1] == affect_class], [float(w[1]) for w in words if w[-1] == affect_class]\n",
    "\n",
    "def run_pplm_example(\n",
    "        pretrained_model=\"gpt2-medium\",\n",
    "        cond_text=\"\",\n",
    "        affect_weight=0.2,\n",
    "        knob = None,\n",
    "        uncond=False,\n",
    "        num_samples=1,\n",
    "        bag_of_words=None,\n",
    "        bag_of_words_affect=None,\n",
    "        discrim=None,\n",
    "        discrim_weights=None,\n",
    "        discrim_meta=None,\n",
    "        class_label=-1,\n",
    "        length=100,\n",
    "        stepsize=0.02,\n",
    "        temperature=1.0,\n",
    "        top_k=10,\n",
    "        sample=True,\n",
    "        num_iterations=3,\n",
    "        grad_length=10000,\n",
    "        horizon_length=1,\n",
    "        window_length=0,\n",
    "        decay=False,\n",
    "        gamma=1.5,\n",
    "        gm_scale=0.9,\n",
    "        score_scale=0.01,\n",
    "        seed=0,\n",
    "        no_cuda=False,\n",
    "        colorama=False,\n",
    "        verbosity='regular',\n",
    "        beta1=0.6,\n",
    "        end_lr = 0.5,\n",
    "        N = 15,\n",
    "        power = 2\n",
    "        ):\n",
    "    # set Random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # set verbosiry\n",
    "    verbosity_level = VERBOSITY_LEVELS.get(verbosity.lower(), REGULAR)\n",
    "\n",
    "    # # set the device\n",
    "    device = \"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
    "    # device = \"cpu\"\n",
    "\n",
    "    if discrim == 'generic':\n",
    "        set_generic_model_params(discrim_weights, discrim_meta)\n",
    "\n",
    "    if discrim is not None:\n",
    "        discriminator_pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim][\n",
    "            \"pretrained_model\"\n",
    "        ]\n",
    "        if pretrained_model != discriminator_pretrained_model:\n",
    "            pretrained_model = discriminator_pretrained_model\n",
    "            if verbosity_level >= REGULAR:\n",
    "                print(\"discrim = {}, pretrained_model set \"\n",
    "                \"to discriminator's = {}\".format(discrim, pretrained_model))\n",
    "\n",
    "    # load pretrained model\n",
    "    model = GPT2LMHeadModel.from_pretrained(\n",
    "        pretrained_model,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # load tokenizer\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "    # Freeze GPT-2 weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # figure out conditioning text\n",
    "    if uncond:\n",
    "        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token],add_special_tokens=False)\n",
    "    else:\n",
    "        raw_text = cond_text\n",
    "        while not raw_text:\n",
    "            print(\"Did you forget to add `--cond_text`? \")\n",
    "            raw_text = input(\"Model prompt >>> \")\n",
    "        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text,add_special_tokens=False)\n",
    "    print(\"= Prefix of sentence =\")\n",
    "    # generate unperturbed and perturbed texts\n",
    "\n",
    "    # full_text_generation returns:\n",
    "    # unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
    "    unpert_gen_tok_text, pert_gen_tok_texts, _, _ = full_text_generation(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        affect_weight=affect_weight,\n",
    "        knob = knob,\n",
    "        context=tokenized_cond_text,\n",
    "        device=device,\n",
    "        num_samples=num_samples,\n",
    "        bag_of_words=bag_of_words,\n",
    "        bag_of_words_affect=bag_of_words_affect,\n",
    "        discrim=discrim,\n",
    "        class_label=class_label,\n",
    "        length=length,\n",
    "        stepsize=stepsize,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        sample=sample,\n",
    "        num_iterations=num_iterations,\n",
    "        grad_length=grad_length,\n",
    "        horizon_length=horizon_length,\n",
    "        window_length=window_length,\n",
    "        decay=decay,\n",
    "        gamma=gamma,\n",
    "        gm_scale=gm_scale,\n",
    "        score_scale=score_scale,\n",
    "        verbosity_level=verbosity_level,\n",
    "        beta1=beta1,\n",
    "        end_lr = end_lr,\n",
    "        N = N,\n",
    "        power = power\n",
    "    )\n",
    "\n",
    "    # untokenize unperturbed text\n",
    "    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n",
    "\n",
    "    if verbosity_level >= REGULAR:\n",
    "        print(\"=\" * 80)\n",
    "    print(\"= Unperturbed generated text =\")\n",
    "    print(unpert_gen_text)\n",
    "    print()\n",
    "\n",
    "    generated_texts = []\n",
    "\n",
    "    # iterate through the perturbed texts\n",
    "    for i, pert_gen_tok_text in enumerate(pert_gen_tok_texts):\n",
    "        try:\n",
    "            # untokenize unperturbed text\n",
    "            pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n",
    "            # print(\"= Perturbed generated text {} =\".format(i + 1))\n",
    "            # print(pert_gen_text)\n",
    "            # print()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # keep the prefix, perturbed seq, original seq for each index\n",
    "        generated_texts.append(\n",
    "            (tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text)\n",
    "        )\n",
    "\n",
    "    return pert_gen_text\n",
    "\n",
    "def generate_text_pplm(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        affect_weight=0.2,\n",
    "        context=None,\n",
    "        past=None,\n",
    "        device=\"cuda\",\n",
    "        perturb=True,\n",
    "        bow_indices=None,\n",
    "        bow_indices_affect=None,\n",
    "        affect_int = None,\n",
    "        knob = None,\n",
    "        classifier=None,\n",
    "        class_label=None,\n",
    "        loss_type=0,\n",
    "        length=100,\n",
    "        stepsize=0.02,\n",
    "        temperature=1.0,\n",
    "        top_k=10,\n",
    "        sample=True,\n",
    "        num_iterations=3,\n",
    "        grad_length=10000,\n",
    "        horizon_length=1,\n",
    "        window_length=0,\n",
    "        decay=False,\n",
    "        gamma=1.5,\n",
    "        gm_scale=0.9,\n",
    "        score_scale=0.01,\n",
    "        verbosity_level=REGULAR,\n",
    "        beta1=0.6,\n",
    "        end_lr = 0.5,\n",
    "        N = 15,\n",
    "        power = 2\n",
    "):\n",
    "    output_so_far = None\n",
    "    if context:\n",
    "        context_t = torch.tensor(context, device=device, dtype=torch.long)\n",
    "        while len(context_t.shape) < 2:\n",
    "            context_t = context_t.unsqueeze(0)\n",
    "        output_so_far = context_t\n",
    "\n",
    "    # collect one hot vectors for bags of words\n",
    "    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\n",
    "    affect_int_orig = affect_int\n",
    "    one_hot_bows_affect, affect_int = build_bows_one_hot_vectors_aff(bow_indices_affect, affect_int, tokenizer, device)\n",
    "#    print(torch.FloatTensor(one_hot_bows_affect).size())\n",
    "    grad_norms = None\n",
    "    last = None\n",
    "    unpert_discrim_loss = 0\n",
    "    loss_in_time = []\n",
    "\n",
    "    if verbosity_level >= VERBOSE:\n",
    "        range_func = trange(length, ascii=True)\n",
    "    else:\n",
    "        range_func = range(length)\n",
    "    count = 0\n",
    "    int_score = 0\n",
    "    for i in range_func:\n",
    "        if count == 2:\n",
    "          break\n",
    "        # Get past/probs for current output, except for last word\n",
    "        # Note that GPT takes 2 inputs: past + current_token\n",
    "\n",
    "        # run model forward to obtain unperturbed\n",
    "        if past is None and output_so_far is not None:\n",
    "            last = output_so_far[:, -1:]\n",
    "            if output_so_far.shape[1] > 1:\n",
    "                _, past, _ = model(output_so_far[:, :-1])\n",
    "\n",
    "        unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\n",
    "        unpert_last_hidden = unpert_all_hidden[-1]\n",
    "\n",
    "        # check if we are abowe grad max length\n",
    "        if i >= grad_length:\n",
    "            current_stepsize = stepsize * 0\n",
    "        else:\n",
    "            current_stepsize = stepsize\n",
    "\n",
    "        # modify the past if necessary\n",
    "        if not perturb or num_iterations == 0:\n",
    "            pert_past = past\n",
    "\n",
    "        else:\n",
    "            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n",
    "            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n",
    "\n",
    "            if past is not None:\n",
    "                pert_past, _, grad_norms, loss_this_iter = perturb_past(\n",
    "                    past,\n",
    "                    model,\n",
    "                    last,\n",
    "                    affect_weight = affect_weight,\n",
    "                    unpert_past=unpert_past,\n",
    "                    unpert_logits=unpert_logits,\n",
    "                    accumulated_hidden=accumulated_hidden,\n",
    "                    grad_norms=grad_norms,\n",
    "                    stepsize=current_stepsize,\n",
    "                    one_hot_bows_vectors=one_hot_bows_vectors,\n",
    "                    one_hot_bows_affect=one_hot_bows_affect,\n",
    "                    affect_int = affect_int,\n",
    "                    knob = knob,\n",
    "                    classifier=classifier,\n",
    "                    class_label=class_label,\n",
    "                    loss_type=loss_type,\n",
    "                    num_iterations=num_iterations,\n",
    "                    horizon_length=horizon_length,\n",
    "                    window_length=window_length,\n",
    "                    decay=decay,\n",
    "                    gamma=gamma,\n",
    "                    score_scale=score_scale,\n",
    "                    device=device,\n",
    "                    verbosity_level=verbosity_level,\n",
    "                    beta1=beta1,\n",
    "                    end_lr = end_lr,\n",
    "                    N = N,\n",
    "                    power = power\n",
    "                )\n",
    "                loss_in_time.append(loss_this_iter)\n",
    "            else:\n",
    "                pert_past = past\n",
    "\n",
    "        pert_logits, past, pert_all_hidden = model(last, past=pert_past)\n",
    "        pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\n",
    "        pert_probs = F.softmax(pert_logits, dim=-1)\n",
    "\n",
    "        if classifier is not None:\n",
    "            ce_loss = torch.nn.CrossEntropyLoss()\n",
    "            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n",
    "            label = torch.tensor([class_label], device=device,\n",
    "                                 dtype=torch.long)\n",
    "            unpert_discrim_loss = ce_loss(prediction, label)\n",
    "            if verbosity_level >= VERBOSE:\n",
    "                print(\n",
    "                    \"unperturbed discrim loss\",\n",
    "                    unpert_discrim_loss.data.cpu().numpy()\n",
    "                )\n",
    "        else:\n",
    "            unpert_discrim_loss = 0\n",
    "\n",
    "        # Fuse the modified model and original model\n",
    "        if perturb:\n",
    "\n",
    "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
    "\n",
    "            pert_probs = ((pert_probs ** gm_scale) * (\n",
    "                    unpert_probs ** (1 - gm_scale)))  # + SMALL_CONST\n",
    "            pert_probs = top_k_filter(pert_probs, k=top_k,\n",
    "                                      probs=True)  # + SMALL_CONST\n",
    "\n",
    "            # rescale\n",
    "            if torch.sum(pert_probs) <= 1:\n",
    "                pert_probs = pert_probs / torch.sum(pert_probs)\n",
    "\n",
    "        else:\n",
    "            pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\n",
    "            pert_probs = F.softmax(pert_logits, dim=-1)\n",
    "\n",
    "        # sample or greedy\n",
    "        if sample:\n",
    "            last = torch.multinomial(pert_probs, num_samples=1)\n",
    "            # print('pert_prob, last ', pert_probs, last)\n",
    "\n",
    "        else:\n",
    "            _, last = torch.topk(pert_probs, k=1, dim=-1)\n",
    "\n",
    "        # update context/output_so_far appending the new token\n",
    "        output_so_far = (\n",
    "            last if output_so_far is None\n",
    "            else torch.cat((output_so_far, last), dim=1)\n",
    "        )\n",
    "\n",
    "        resultContainer[\"text\"].append(tokenizer.decode(output_so_far.tolist()[0])[-1])\n",
    "        \n",
    "        if verbosity_level >= REGULAR:\n",
    "            print(tokenizer.decode(output_so_far.tolist()[0]))\n",
    "        if(tokenizer.decode(output_so_far.tolist()[0])[-1] == '.' ):\n",
    "          count = count+1\n",
    "        if bow_indices_affect is not None and [output_so_far.tolist()[0][-1]] in bow_indices_affect[0]:\n",
    "          int_word = affect_int_orig[bow_indices_affect[0].index([output_so_far.tolist()[0][-1]])]\n",
    "          print(tokenizer.decode(output_so_far.tolist()[0][-1]), int_word)\n",
    "          int_score = int_score + int_word\n",
    "    print(\"int_score: \", int_score)\n",
    "    # print(\"int.. \" , output_so_far.tolist()[0][-1])\n",
    "    return output_so_far, unpert_discrim_loss, loss_in_time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_generic_model_params(discrim_weights, discrim_meta):\n",
    "    if discrim_weights is None:\n",
    "        raise ValueError('When using a generic discriminator, '\n",
    "                         'discrim_weights need to be specified')\n",
    "    if discrim_meta is None:\n",
    "        raise ValueError('When using a generic discriminator, '\n",
    "                         'discrim_meta need to be specified')\n",
    "\n",
    "    with open(discrim_meta, 'r') as discrim_meta_file:\n",
    "        meta = json.load(discrim_meta_file)\n",
    "    meta['path'] = discrim_weights\n",
    "    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta\n",
    "\n",
    "# # set the device\n",
    "# device = \"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"legal\",'military','monsters', 'science','space','politics', 'religion','technology','positive_words']\n",
    "affects = ['anticipation', 'disgust', 'surprise', 'trust']#['joy', 'anger', 'sadness','fear']\n",
    "prompts = [ 'The book', 'The issue focused on', 'The robots', 'The relationship','The road']\n",
    "knob_vals = [0.1,0.5,1]\n",
    "outputs = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "  for topic in topics:\n",
    "    for affect in affects:\n",
    "      for knob in knob_vals:\n",
    "        print(\"topic:\", topic, \", affect:\", affect, \", knob is:\", knob)\n",
    "        \n",
    "        \n",
    "        output = run_pplm_example(\n",
    "            affect_weight=1, \n",
    "            knob = knob, # 0-1\n",
    "            cond_text=prompt,\n",
    "            num_samples=1,\n",
    "            bag_of_words=topic,\n",
    "            bag_of_words_affect=affect,\n",
    "            length=50,\n",
    "            stepsize=8e-4, #topic, affect convergence rate\n",
    "            sample=True,\n",
    "            num_iterations=40,\n",
    "            window_length=6,\n",
    "            gamma=1.5,\n",
    "            gm_scale=0.95,\n",
    "            score_scale=1,\n",
    "            verbosity='REGULAR',\n",
    "            end_lr = 1e-4,\n",
    "            N = 10,\n",
    "            power = 2\n",
    "        )\n",
    "        print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "affected_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3d7b3b8757ddcb85e009e91c7aaa3869ff10310ffa06b1e46dd6deee21417d0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
